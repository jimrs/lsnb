{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "import math\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_X_y, check_array, deprecated\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from scipy.special import logsumexp\n",
    "from sklearn.utils.multiclass import _check_partial_fit_first_call\n",
    "from sklearn.utils.validation import check_is_fitted, check_non_negative, column_or_1d\n",
    "from sklearn.utils.validation import _check_sample_weight\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer, load_iris\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class _BaseNB(ClassifierMixin, BaseEstimator, metaclass=ABCMeta):\n",
    "    \"\"\"Abstract base class for naive Bayes estimators\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"Compute the unnormalized posterior log probability of X\n",
    "        I.e. ``log P(c) + log P(x|c)`` for all rows x of X, as an array-like of\n",
    "        shape [n_classes, n_samples].\n",
    "        Input is passed to _joint_log_likelihood as-is by predict,\n",
    "        predict_proba and predict_log_proba.\n",
    "        \"\"\"\n",
    "\n",
    "    def _check_X(self, X):\n",
    "        \"\"\"To be overridden in subclasses with the actual checks.\"\"\"\n",
    "        # Note that this is not marked @abstractmethod as long as the\n",
    "        # deprecated public alias sklearn.naive_bayes.BayesNB exists\n",
    "        # (until 0.24) to preserve backward compat for 3rd party projects\n",
    "        # with existing derived classes.\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform classification on an array of test vectors X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        C : ndarray of shape (n_samples,)\n",
    "            Predicted target values for X\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        X = self._check_X(X)\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        return self.classes_[np.argmax(jll, axis=1)]\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return log-probability estimates for the test vector X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        C : array-like of shape (n_samples, n_classes)\n",
    "            Returns the log-probability of the samples for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute :term:`classes_`.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        X = self._check_X(X)\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        # normalize by P(x) = P(f_1, ..., f_n)\n",
    "        log_prob_x = logsumexp(jll, axis=1)\n",
    "        return jll - np.atleast_2d(log_prob_x).T\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return probability estimates for the test vector X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        C : array-like of shape (n_samples, n_classes)\n",
    "            Returns the probability of the samples for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute :term:`classes_`.\n",
    "        \"\"\"\n",
    "        return np.exp(self.predict_log_proba(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "_ALPHA_MIN = 1e-10\n",
    "\n",
    "class _BaseDiscreteNB(_BaseNB):\n",
    "    \"\"\"Abstract base class for naive Bayes on discrete/categorical data\n",
    "    Any estimator based on this class should provide:\n",
    "    __init__\n",
    "    _joint_log_likelihood(X) as per _BaseNB\n",
    "    \"\"\"\n",
    "\n",
    "    def _check_X(self, X):\n",
    "        return check_array(X, accept_sparse='csr')\n",
    "\n",
    "    def _check_X_y(self, X, y):\n",
    "        return check_X_y(X, y, accept_sparse='csr')\n",
    "\n",
    "    def _update_class_log_prior(self, class_prior=None):\n",
    "        n_classes = len(self.classes_)\n",
    "        if class_prior is not None:\n",
    "            if len(class_prior) != n_classes:\n",
    "                raise ValueError(\"Number of priors must match number of\"\n",
    "                                 \" classes.\")\n",
    "            self.class_log_prior_ = np.log(class_prior)\n",
    "  \n",
    "        elif self.fit_prior:\n",
    "            with warnings.catch_warnings():\n",
    "                # silence the warning when count is 0 because class was not yet\n",
    "                # observed\n",
    "                warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "                log_class_count = np.log(self.class_count_)\n",
    "\n",
    "            # empirical prior, with sample_weight taken into account\n",
    "            self.class_log_prior_ = (log_class_count -\n",
    "                                     np.log(self.class_count_.sum()))\n",
    "            \n",
    "        else:\n",
    "            self.class_log_prior_ = np.full(n_classes, -np.log(n_classes))\n",
    "\n",
    "    def _check_alpha(self):\n",
    "        if np.min(self.alpha) < 0:\n",
    "            raise ValueError('Smoothing parameter alpha = %.1e. '\n",
    "                             'alpha should be > 0.' % np.min(self.alpha))\n",
    "        if isinstance(self.alpha, np.ndarray):\n",
    "            if not self.alpha.shape[0] == self.n_features_:\n",
    "                raise ValueError(\"alpha should be a scalar or a numpy array \"\n",
    "                                 \"with shape [n_features]\")\n",
    "        if np.min(self.alpha) < _ALPHA_MIN:\n",
    "            warnings.warn('alpha too small will result in numeric errors, '\n",
    "                          'setting alpha = %.1e' % _ALPHA_MIN)\n",
    "            return np.maximum(self.alpha, _ALPHA_MIN)\n",
    "        return self.alpha\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        X, y = self._check_X_y(X, y)\n",
    "        _, n_features = X.shape\n",
    "        self.n_features_ = n_features\n",
    "\n",
    "        labelbin = LabelBinarizer()\n",
    "        Y = labelbin.fit_transform(y)\n",
    "        self.classes_ = labelbin.classes_\n",
    "        if Y.shape[1] == 1:\n",
    "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
    "\n",
    "        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n",
    "        # We convert it to np.float64 to support sample_weight consistently;\n",
    "        # this means we also don't have to cast X to floating point\n",
    "        if sample_weight is not None:\n",
    "            Y = Y.astype(np.float64, copy=False)\n",
    "            sample_weight = np.asarray(sample_weight)\n",
    "            sample_weight = np.atleast_2d(sample_weight)\n",
    "            Y *= check_array(sample_weight).T\n",
    "\n",
    "        class_prior = self.class_prior\n",
    "\n",
    "        # Count raw events from data before updating the class log prior\n",
    "        # and feature log probas\n",
    "        n_effective_classes = Y.shape[1]\n",
    "\n",
    "        self._init_counters(n_effective_classes, n_features)\n",
    "        # added y here as a parameter, to be used in LSNB implementation of _count()\n",
    "        self._count(X, Y, y)\n",
    "        alpha = self._check_alpha()\n",
    "        self._update_feature_log_prob(alpha)\n",
    "        self._update_class_log_prior(class_prior=class_prior)\n",
    "        return self\n",
    "\n",
    "    def _init_counters(self, n_effective_classes, n_features):\n",
    "        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
    "        self.feature_count_ = np.zeros((n_effective_classes, n_features),\n",
    "                                       dtype=np.float64)\n",
    "\n",
    "    # XXX The following is a stopgap measure; we need to set the dimensions\n",
    "    # of class_log_prior_ and feature_log_prob_ correctly.\n",
    "    def _get_coef(self):\n",
    "        return (self.feature_log_prob_[1:]\n",
    "                if len(self.classes_) == 2 else self.feature_log_prob_)\n",
    "\n",
    "    def _get_intercept(self):\n",
    "        return (self.class_log_prior_[1:]\n",
    "                if len(self.classes_) == 2 else self.class_log_prior_)\n",
    "\n",
    "    coef_ = property(_get_coef)\n",
    "    intercept_ = property(_get_intercept)\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {'poor_score': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LooselySymmetricNB(_BaseDiscreteNB):\n",
    "    \n",
    "    def __init__(self, alpha=1.0, fit_prior=True, class_prior=None, enhance=False):\n",
    "        self.alpha = alpha\n",
    "        self.fit_prior = fit_prior\n",
    "        self.class_prior = class_prior\n",
    "        self.enhance = enhance\n",
    "        \n",
    "    def _count(self, X, Y, y):\n",
    "        \"\"\"Count and smooth feature occurrences.\"\"\"\n",
    "        \n",
    "        check_non_negative(X, \"LooselySymmetricNB (input X)\")\n",
    "        self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
    "        self.class_count_ += Y.sum(axis=0)\n",
    "        \n",
    "        # we need these two values in order to calculate document frequency in _calculate_df()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def _update_feature_log_prob(self, alpha):\n",
    "\n",
    "        self.smoothed_fc = self.feature_count_ + alpha\n",
    "        self.smoothed_cc = self.smoothed_fc.sum(axis=1)\n",
    "        \n",
    "        self._calculate_df()\n",
    "        self._calculate_abcd(self.smoothed_fc, self.smoothed_cc.reshape(-1, 1), self.enhance)\n",
    "        \n",
    "        # HAM\n",
    "        self.bd = (self.b * self.d) / (self.b + self.d)\n",
    "        self.ac = (self.a * self.c) / (self.a + self.c)\n",
    "        bd = (self.b * self.d) / (self.b + self.d)\n",
    "        ac = (self.a * self.c) / (self.a + self.c)\n",
    "        numerator = self.a + bd\n",
    "        denumerator = self.a + self.b + ac + bd\n",
    "        \n",
    "        # index 0 is for ham, index 1 is for spam\n",
    "        self.feature_log_prob_ = np.empty(self.feature_count_.shape) \n",
    "        self.feature_log_prob_[0] = np.log(numerator) - np.log(denumerator)\n",
    "        \n",
    "        # SPAM\n",
    "        numerator = self.c + bd\n",
    "        denumerator = self.c + self.d + ac + bd\n",
    "        \n",
    "        self.feature_log_prob_[1] = np.log(numerator) - np.log(denumerator)\n",
    "    \n",
    "    def _calculate_df(self):\n",
    "        \n",
    "        self.df = np.zeros(self.feature_count_.shape, dtype=np.int32)\n",
    "        for mail_idx, mail in enumerate(self.X):\n",
    "            for word_idx, word in enumerate(mail):\n",
    "                if word >= 1:\n",
    "                    self.df[self.y[mail_idx]][word_idx] += 1\n",
    "    \n",
    "    def _calculate_abcd(self, fc, cc, enhance):\n",
    "        \n",
    "        # at 0 is ham info, at 1 is spam info\n",
    "        if enhance:\n",
    "            word_density_ham = fc[0] / cc[0]\n",
    "            word_density_spam = fc[1] / cc[1]\n",
    "        \n",
    "        else:\n",
    "            word_density_ham = 1\n",
    "            word_density_spam = 1\n",
    "        \n",
    "        self.a = (self.df[0] / self.class_count_[0]) * word_density_ham\n",
    "        self.b = (1 - self.a) * word_density_spam\n",
    "        self.c = (self.df[1] / self.class_count_[1]) * word_density_spam\n",
    "        self.d = (1 - self.c) * word_density_ham\n",
    "        \n",
    "        \n",
    "    def _joint_log_likelihood(self, X):\n",
    "       \n",
    "        return (safe_sparse_dot(X, self.feature_log_prob_.T) + \n",
    "                self.class_log_prior_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = load_breast_cancer(return_X_y=True)\n",
    "# data, labels = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.481e+01 1.470e+01 9.466e+01 6.807e+02 8.472e-02 5.016e-02 3.416e-02\n",
      "  2.541e-02 1.659e-01 5.348e-02 2.182e-01 6.232e-01 1.677e+00 2.072e+01\n",
      "  6.708e-03 1.197e-02 1.482e-02 1.056e-02 1.580e-02 1.779e-03 1.561e+01\n",
      "  1.758e+01 1.017e+02 7.602e+02 1.139e-01 1.011e-01 1.101e-01 7.955e-02\n",
      "  2.334e-01 6.142e-02]\n",
      " [1.169e+01 2.444e+01 7.637e+01 4.064e+02 1.236e-01 1.552e-01 4.515e-02\n",
      "  4.531e-02 2.131e-01 7.405e-02 2.957e-01 1.978e+00 2.158e+00 2.095e+01\n",
      "  1.288e-02 3.495e-02 1.865e-02 1.766e-02 1.560e-02 5.824e-03 1.298e+01\n",
      "  3.219e+01 8.612e+01 4.877e+02 1.768e-01 3.251e-01 1.395e-01 1.308e-01\n",
      "  2.803e-01 9.970e-02]\n",
      " [1.530e+01 2.527e+01 1.024e+02 7.324e+02 1.082e-01 1.697e-01 1.683e-01\n",
      "  8.751e-02 1.926e-01 6.540e-02 4.390e-01 1.012e+00 3.498e+00 4.350e+01\n",
      "  5.233e-03 3.057e-02 3.576e-02 1.083e-02 1.768e-02 2.967e-03 2.027e+01\n",
      "  3.671e+01 1.493e+02 1.269e+03 1.641e-01 6.110e-01 6.335e-01 2.024e-01\n",
      "  4.027e-01 9.876e-02]\n",
      " [1.916e+01 2.660e+01 1.262e+02 1.138e+03 1.020e-01 1.453e-01 1.921e-01\n",
      "  9.664e-02 1.902e-01 6.220e-02 6.361e-01 1.001e+00 4.321e+00 6.965e+01\n",
      "  7.392e-03 2.449e-02 3.988e-02 1.293e-02 1.435e-02 3.446e-03 2.372e+01\n",
      "  3.590e+01 1.598e+02 1.724e+03 1.782e-01 3.841e-01 5.754e-01 1.872e-01\n",
      "  3.258e-01 9.720e-02]\n",
      " [1.627e+01 2.071e+01 1.069e+02 8.137e+02 1.169e-01 1.319e-01 1.478e-01\n",
      "  8.488e-02 1.948e-01 6.277e-02 4.375e-01 1.232e+00 3.270e+00 4.441e+01\n",
      "  6.697e-03 2.083e-02 3.248e-02 1.392e-02 1.536e-02 2.789e-03 1.928e+01\n",
      "  3.038e+01 1.298e+02 1.121e+03 1.590e-01 2.947e-01 3.597e-01 1.583e-01\n",
      "  3.103e-01 8.200e-02]]\n",
      "[1 1 0 0 0 1 0 0 0 0]\n",
      "[1 1 0 0 0 1 0 0 1 0]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "bnb.fit(X_train, y_train)\n",
    "print(X_test[:5])\n",
    "print(y_test[:10])\n",
    "\n",
    "print(mnb.predict(X_test[:10]))\n",
    "print(bnb.predict(X_test[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.7 3.2 1.6 0.2]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [7.7 3.  6.1 2.3]]\n",
      "[0 0 0 1 2]\n",
      "[0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jim/code/lsnb/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/jim/code/lsnb/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/jim/code/lsnb/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:44: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/jim/code/lsnb/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/jim/code/lsnb/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/jim/code/lsnb/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/jim/code/lsnb/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(LooselySymmetricNB()).fit(X_train, y_train)\n",
    "print(X_test[:5])\n",
    "print(y_test[:5])\n",
    "\n",
    "print(clf.predict(X_test[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[165 165 165 165   0   0   0   0   0   0  19 110 165 165   0   0   0   0\n",
      "    0   0 165 165 165 165   0   1   2   0   0   0]\n",
      " [261 261 261 261   0   0   0   0   0   0   0 152 254 261   0   0   0   0\n",
      "    0   0 261 261 261 261   0   0   1   0   0   0]]\n",
      "[1.         1.         1.         1.         0.         0.\n",
      " 0.         0.         0.         0.         0.11515152 0.66666667\n",
      " 1.         1.         0.         0.         0.         0.\n",
      " 0.         0.         1.         1.         1.         1.\n",
      " 0.         0.00606061 0.01212121 0.         0.         0.        ]\n",
      "[0.         0.         0.         0.         1.         1.\n",
      " 1.         1.         1.         1.         0.88484848 0.33333333\n",
      " 0.         0.         1.         1.         1.         1.\n",
      " 1.         1.         0.         0.         0.         0.\n",
      " 1.         0.99393939 0.98787879 1.         1.         1.        ]\n",
      "[1.         1.         1.         1.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.58237548\n",
      " 0.97318008 1.         0.         0.         0.         0.\n",
      " 0.         0.         1.         1.         1.         1.\n",
      " 0.         0.         0.00383142 0.         0.         0.        ]\n",
      "[0.         0.         0.         0.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.41762452\n",
      " 0.02681992 0.         1.         1.         1.         1.\n",
      " 1.         1.         0.         0.         0.         0.\n",
      " 1.         1.         0.99616858 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "# DF se pocita pro vetsi nebo rovno 1, coz u takovychto hodnot dela bordel\n",
    "# vyzkouset jeste to same pro bernoulli/neco takoveho pro podporu argumentu\n",
    "print(clf.df)\n",
    "print(clf.a)\n",
    "print(clf.b)\n",
    "print(clf.c)\n",
    "print(clf.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.296e+01 1.829e+01 8.418e+01 5.252e+02 7.351e-02 7.899e-02 4.057e-02\n",
      "  1.883e-02 1.874e-01 5.899e-02 2.357e-01 1.299e+00 2.397e+00 2.021e+01\n",
      "  3.629e-03 3.713e-02 3.452e-02 1.065e-02 2.632e-02 3.705e-03 1.413e+01\n",
      "  2.461e+01 9.631e+01 6.219e+02 9.329e-02 2.318e-01 1.604e-01 6.608e-02\n",
      "  3.207e-01 7.247e-02]\n",
      " [8.878e+00 1.549e+01 5.674e+01 2.410e+02 8.293e-02 7.698e-02 4.721e-02\n",
      "  2.381e-02 1.930e-01 6.621e-02 5.381e-01 1.200e+00 4.277e+00 3.018e+01\n",
      "  1.093e-02 2.899e-02 3.214e-02 1.506e-02 2.837e-02 4.174e-03 9.981e+00\n",
      "  1.770e+01 6.527e+01 3.020e+02 1.015e-01 1.248e-01 9.441e-02 4.762e-02\n",
      "  2.434e-01 7.431e-02]\n",
      " [1.404e+01 1.598e+01 8.978e+01 6.112e+02 8.458e-02 5.895e-02 3.534e-02\n",
      "  2.944e-02 1.714e-01 5.898e-02 3.892e-01 1.046e+00 2.644e+00 3.274e+01\n",
      "  7.976e-03 1.295e-02 1.608e-02 9.046e-03 2.005e-02 2.830e-03 1.566e+01\n",
      "  2.158e+01 1.012e+02 7.500e+02 1.195e-01 1.252e-01 1.117e-01 7.453e-02\n",
      "  2.725e-01 7.234e-02]\n",
      " [1.120e+01 2.937e+01 7.067e+01 3.860e+02 7.449e-02 3.558e-02 0.000e+00\n",
      "  0.000e+00 1.060e-01 5.502e-02 3.141e-01 3.896e+00 2.041e+00 2.281e+01\n",
      "  7.594e-03 8.878e-03 0.000e+00 0.000e+00 1.989e-02 1.773e-03 1.192e+01\n",
      "  3.830e+01 7.519e+01 4.396e+02 9.267e-02 5.494e-02 0.000e+00 0.000e+00\n",
      "  1.566e-01 5.905e-02]\n",
      " [1.141e+01 1.082e+01 7.334e+01 4.033e+02 9.373e-02 6.685e-02 3.512e-02\n",
      "  2.623e-02 1.667e-01 6.113e-02 1.408e-01 4.607e-01 1.103e+00 1.050e+01\n",
      "  6.040e-03 1.529e-02 1.514e-02 6.460e-03 1.344e-02 2.206e-03 1.282e+01\n",
      "  1.597e+01 8.374e+01 5.105e+02 1.548e-01 2.390e-01 2.102e-01 8.958e-02\n",
      "  3.016e-01 8.523e-02]]\n",
      "[1 1 1 1 1]\n",
      "[0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jim/code/lsnb/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/jim/code/lsnb/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/jim/code/lsnb/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/jim/code/lsnb/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "clf = LooselySymmetricNB()\n",
    "clf.fit(X_train, y_train)\n",
    "print(X_test[:5])\n",
    "print(y_test[:5])\n",
    "\n",
    "print(clf.predict(X_test[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunch = load_breast_cancer()\n",
    "# bunch.target_names\n",
    "bunch.feature_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
