{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOSELY SYMMETRIC NAIVE BAYES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "import math\n",
    "import nltk\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_X_y, check_array, deprecated\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from scipy.special import logsumexp\n",
    "from sklearn.utils.multiclass import _check_partial_fit_first_call\n",
    "from sklearn.utils.validation import check_is_fitted, check_non_negative, column_or_1d\n",
    "from sklearn.utils.validation import _check_sample_weight\n",
    "# from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loaded\n"
    }
   ],
   "source": [
    "mailsDir = \"../datasets/enron1/\"\n",
    "spamDir = os.path.join(mailsDir, \"spam\")\n",
    "hamDir = os.path.join(mailsDir, \"ham\")\n",
    "\n",
    "mails = []\n",
    "spaminfo = []\n",
    "\n",
    "hamDirList = os.listdir(hamDir)\n",
    "for file in hamDirList:\n",
    "    with open(os.path.join(hamDir, file), \"r\", encoding=\"latin-1\") as f:\n",
    "        mail = f.read()\n",
    "        mails.append(mail)\n",
    "        spaminfo.append(0)\n",
    "\n",
    "spamDirList = os.listdir(spamDir)\n",
    "for file in spamDirList:\n",
    "    with open(os.path.join(spamDir, file), \"r\", encoding=\"latin-1\") as f:\n",
    "        mail = f.read()\n",
    "        mails.append(mail)\n",
    "        spaminfo.append(1)\n",
    "\n",
    "# shuffle the dataset, so it is not divided exactly as first 70% ham, other 30% spam\n",
    "ordered = list(zip(mails, spaminfo))\n",
    "random.shuffle(ordered)\n",
    "mails, spaminfo = zip(*ordered)\n",
    "print(\"loaded\")\n",
    "\n",
    "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def tokenize(text, stemmer=porter_stemmer):\n",
    "    lower_text = text.lower()\n",
    "    tokens = nltk.wordpunct_tokenize(lower_text)\n",
    "    stems = [porter_stemmer.stem(token) for token in tokens]\n",
    "    punct_less = [stem for stem in stems if re.match(\n",
    "        '^[a-zA-Z]+$', stem\n",
    "    ) is not None]\n",
    "    return punct_less\n",
    "\n",
    "# stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "# with open(\"./stopwords.txt\", \"w\") as outf:\n",
    "#     outf.write(\"\\n\".join(stopwords))\n",
    "\n",
    "with open(\"../stop_words.txt\", \"r\") as inf:\n",
    "    stopwords = inf.read().splitlines()\n",
    "\n",
    "stop_words = []\n",
    "for word in stopwords:\n",
    "    stop_words.append(tokenize(word)[0])\n",
    "stop_words.append(\"becau\")\n",
    "stop_words = list(dict.fromkeys(stop_words))  # remove duplicates\n",
    "\n",
    "vec = CountVectorizer(\n",
    "    encoding=\"latin-1\",\n",
    "    decode_error=\"replace\",\n",
    "    strip_accents=\"unicode\",\n",
    "    analyzer=\"word\",\n",
    "    binary=False,\n",
    "    stop_words = stop_words,\n",
    "    tokenizer = tokenize,\n",
    "    ngram_range=(1,1),\n",
    "    max_df=0.99,\n",
    "    min_df=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset size: 50\nBernoulliNB accuracy: 0.52\nBernoulliNB precision: 0.3684210526315789\nBernoulliNB recall: 1.0\nBernoulliNB f1 score: 0.5384615384615384\n-----------\nDataset size: 100\nBernoulliNB accuracy: 0.74\nBernoulliNB precision: 0.3333333333333333\nBernoulliNB recall: 0.08333333333333333\nBernoulliNB f1 score: 0.13333333333333333\n-----------\nDataset size: 200\nBernoulliNB accuracy: 0.78\nBernoulliNB precision: 0.8461538461538461\nBernoulliNB recall: 0.3548387096774194\nBernoulliNB f1 score: 0.5\n-----------\nDataset size: 300\nBernoulliNB accuracy: 0.8\nBernoulliNB precision: 0.8666666666666667\nBernoulliNB recall: 0.3170731707317073\nBernoulliNB f1 score: 0.4642857142857143\n-----------\nDataset size: 400\nBernoulliNB accuracy: 0.805\nBernoulliNB precision: 0.8863636363636364\nBernoulliNB recall: 0.5342465753424658\nBernoulliNB f1 score: 0.6666666666666666\n-----------\nDataset size: 500\nBernoulliNB accuracy: 0.856\nBernoulliNB precision: 0.8545454545454545\nBernoulliNB recall: 0.6266666666666667\nBernoulliNB f1 score: 0.7230769230769231\n-----------\nDataset size: 600\nBernoulliNB accuracy: 0.8433333333333334\nBernoulliNB precision: 0.8947368421052632\nBernoulliNB recall: 0.5543478260869565\nBernoulliNB f1 score: 0.6845637583892618\n-----------\n"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "vec.binary = True\n",
    "\n",
    "for size in [50, 100, 200, 300, 400, 500, 600]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(mails[:size], spaminfo[:size], test_size = 0.5)\n",
    "    \n",
    "    # Count vectorizer\n",
    "    X_train_count = vec.fit_transform(X_train)\n",
    "    X_test_count = vec.transform(X_test)\n",
    "\n",
    "    bnb = BernoulliNB(class_prior=[0.5, 0.5])\n",
    "    bnb.fit(X_train_count, y_train)\n",
    "    \n",
    "    print(f\"Dataset size: {X_train_count.shape[0] *2}\")\n",
    "    \n",
    "    print(f\"BernoulliNB accuracy: {bnb.score(X_test_count.toarray(), y_test)}\")\n",
    "    print(f\"BernoulliNB precision: {precision_score(y_test, bnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"BernoulliNB recall: {recall_score(y_test, bnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"BernoulliNB f1 score: {f1_score(y_test, bnb.predict(X_test_count.toarray()))}\")\n",
    "    print(\"-----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB, MultinomialNB, LSNB, eLSNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-9e53c4e4363a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLooselySymmetricNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0melsnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLooselySymmetricNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menhance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0melsnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/lsnb/lsnb/BaseDiscreteNB.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_class_log_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/lsnb/lsnb/LooselySymmetricNB.py\u001b[0m in \u001b[0;36m_update_feature_log_prob\u001b[0;34m(self, alpha)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothed_cc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothed_fc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_abcd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothed_fc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothed_cc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menhance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/lsnb/lsnb/LooselySymmetricNB.py\u001b[0m in \u001b[0;36m_calculate_df\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmail_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_abcd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menhance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/lsnb/.venv/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnz\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             raise ValueError(\"The truth value of an array with more than one \"\n\u001b[0m\u001b[1;32m    284\u001b[0m                              \"element is ambiguous. Use a.any() or a.all().\")\n\u001b[1;32m    285\u001b[0m     \u001b[0m__nonzero__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from lsnb.LooselySymmetricNB import LooselySymmetricNB\n",
    "vec.binary = False\n",
    "\n",
    "for size in [50, 100, 200, 300, 400, 500, 600]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(mails[:size], spaminfo[:size], test_size = 0.5)\n",
    "    \n",
    "    # Count vectorizer\n",
    "    X_train_count = vec.fit_transform(X_train)\n",
    "    X_test_count = vec.transform(X_test)\n",
    "    \n",
    "    gnb = GaussianNB(priors=[0.5, 0.5])\n",
    "    gnb.fit(X_train_count.toarray(), y_train)\n",
    "    mnb = MultinomialNB(class_prior=[0.5, 0.5])\n",
    "    mnb.fit(X_train_count, y_train)\n",
    "    clf = LooselySymmetricNB(class_prior=[0.5, 0.5])\n",
    "    clf.fit(X_train_count, y_train)\n",
    "    elsnb = LooselySymmetricNB(class_prior=[0.5, 0.5], enhance=True)\n",
    "    elsnb.fit(X_train_count, y_train)\n",
    "\n",
    "    print(f\"Dataset size: {X_train_count.shape[0] * 2}\")\n",
    "    print(f\"LSNB accuracy: {lsnb.score(X_test_count.toarray(), y_test)}\")\n",
    "    print(f\"LSNB precision: {precision_score(y_test, lsnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"LSNB recall: {recall_score(y_test, lsnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"LSNB f1 score: {f1_score(y_test, lsnb.predict(X_test_count.toarray()))}\")\n",
    "    print(\"-----------\")\n",
    "    \n",
    "    print(f\"eLSNB accuracy: {elsnb.score(X_test_count.toarray(), y_test)}\")\n",
    "    print(f\"eLSNB precision: {precision_score(y_test, elsnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"eLSNB recall: {recall_score(y_test, elsnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"eLSNB f1 score: {f1_score(y_test, elsnb.predict(X_test_count.toarray()))}\")\n",
    "    print(\"-----------\")\n",
    "    \n",
    "    print(f\"GaussianNB accuracy: {gnb.score(X_test_count.toarray(), y_test)}\")\n",
    "    print(f\"GaussianNB precision: {precision_score(y_test, gnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"GaussianNB recall: {recall_score(y_test, gnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"GaussianNB f1 score: {f1_score(y_test, gnb.predict(X_test_count.toarray()))}\")\n",
    "    print(\"-----------\")\n",
    "\n",
    "    print(f\"MultinomialNB accuracy: {mnb.score(X_test_count.toarray(), y_test)}\")\n",
    "    print(f\"MultinomialNB precision: {precision_score(y_test, mnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"MultinomialNB recall: {recall_score(y_test, mnb.predict(X_test_count.toarray()))}\")\n",
    "    print(f\"MultinomialNB f1 score: {f1_score(y_test, mnb.predict(X_test_count.toarray()))}\")\n",
    "    print(\"==============\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}